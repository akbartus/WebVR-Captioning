# Image Captioning in Web VR
![Screenshot](assets/screenshot.jpg)

### **Description / Rationale**
This is small project, which shows the use of image captioning (machine learning task, https://huggingface.co/nlpconnect/vit-gpt2-image-captioning) as used in Web VR. It was inspired by similar project created by [Misslivirose](https://github.com/misslivirose) titled  [Scene Reader](https://github.com/misslivirose/scene-reader  ) 




### **Instructions**
Click on The reading platform offers 4 modes for reading: 

**1. User-generated reading environment (Simple Mode)** 
In this mode users can use their own 360 photos or choose the default 3D model of a room or select one of five 3d environments and then upload a pdf file. The respective sound will be added automatically based on random selection (for now there are three types of sound: arts, tale and culture). In addition, there is a possibility to apply weather effects and toggle sound.

![Mode 1](assets/mode1.png)

### **Credits**
<p>3D model of the room (used in simple mode and interactive mode) was created by <b>Francesco
          Coldesina</b>, and taken from <a
          href="https://sketchfab.com/3d-models/big-room-0b5da073be88481091dbef7e55f1d180">Sketchfab.com</a></p>
### **Demo**
To see the application at work: [Demo application](https://www.vr-reader.com)
